# SecureMNIST
Secure Federated Learning Project using MNIST dataset - B.Tech Data Science
# ğŸ” Secure Federated Learning on MNIST Dataset

![GitHub repo size](https://img.shields.io/github/repo-size/Basavavani/secureMNIST)
![GitHub stars](https://img.shields.io/github/stars/Basavavani/secureMNIST?style=social)
![GitHub forks](https://img.shields.io/github/forks/Basavavani/secureMNIST?style=social)
![Last Commit](https://img.shields.io/github/last-commit/Basavavani/secureMNIST)
![License](https://img.shields.io/github/license/Basavavani/secureMNIST)

A privacy-preserving machine learning project that simulates **Federated Learning** using the **MNIST dataset** across multiple virtual clients without sharing raw data.

---

## ğŸ“Œ Project Overview

This project demonstrates how federated learning works by training local models on two separate datasets (simulating two clients) and combining their updates using **Federated Averaging** (FedAvg) â€” all while ensuring data privacy.

---

## ğŸš€ Features

- Federated training using two clients
- Local training with private datasets
- Federated Averaging algorithm
- Accuracy & loss reporting
- Trained model saved as `global_model.pth`

---

## ğŸ§° Tech Stack

- Python 3.9  
- PyTorch  
- Torchvision  
- Visual Studio Code  

---

## ğŸ—‚ï¸ Project Structure

```
secureMNIST/
â”œâ”€â”€ train.py                 # Main training script
â”œâ”€â”€ global_model.pth         # Trained global model
â”œâ”€â”€ data/                    # MNIST dataset
â””â”€â”€ README.md                # Project documentation
```

---

## ğŸ§ª How It Works

1. Load and split the MNIST dataset into two equal parts.
2. Each client trains a local model on its data.
3. Use **FedAvg** to average weights and build a global model.
4. Evaluate the global modelâ€™s performance.

---

## ğŸ§‘â€ğŸ’» Usage

### âœ… Requirements

```bash
pip install torch==1.10.0+cpu torchvision==0.11.1+cpu
```

### â–¶ï¸ Run the Training

```bash
python train.py
```

### ğŸ“¦ Output Example

```
ğŸ”’ Starting Secure Federated MNIST Training...
ğŸ‹ï¸ Training on Client 1...
ğŸ‹ï¸ Training on Client 2...
ğŸ”— Performing Federated Averaging...
âœ… Federated Averaging Done!
ğŸ‰ Secure Federated Training Completed.
âœ… Accuracy: 72.16%
ğŸ“‰ Average Loss: 1.5850
ğŸ’¾ Global model saved as 'global_model.pth'
```

---

## ğŸ“¸ Screenshots

> _You can include VS Code training output, data split view, or loss graphs here._

---

## ğŸ” Real-Life Use Case (Healthcare)

Hospitals in different cities want to collaborate on building a diabetes detection model, but cannot share patient data due to privacy laws.

âœ… With **Federated Learning**, each hospital trains a model locally and shares only model updates â€” not sensitive data. These are averaged to build a global AI model collaboratively.

---

## ğŸ“ˆ Future Improvements

- Extend to multiple clients
- Add secure aggregation with PySyft or CrypTen
- Visualize training accuracy in real-time
- Apply on medical, finance, or industrial datasets

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€“ see the [LICENSE](LICENSE) file for details.

---

> Built with â¤ï¸ by [Basavavani](https://github.com/Basavavani)
